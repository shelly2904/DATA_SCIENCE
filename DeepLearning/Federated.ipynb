{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "054F1u7xVzAM"
      },
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense\n",
        "\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oBRpcqWfOmG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e777256-bc3b-4783-c7ab-4f088fcb4c10"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_digits, train_labels), (test_digits, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIyGFIbofOjb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98fd33d8-d0fd-47c5-b425-17b1d5bc043c"
      },
      "source": [
        "# some variables...\n",
        "image_height = train_digits.shape[1]  \n",
        "image_width = train_digits.shape[2]\n",
        "num_channels = 1  # we have grayscale images\n",
        "# NOTE: image_height == image_width == 28\n",
        "\n",
        "# re-shape the images data\n",
        "train_data = np.reshape(train_digits, (train_digits.shape[0], image_height, image_width, num_channels))\n",
        "test_data = np.reshape(test_digits, (test_digits.shape[0],image_height, image_width, num_channels))\n",
        "\n",
        "# re-scale the image data to values between (0.0,1.0]\n",
        "train_data = train_data.astype('float32') / 255.\n",
        "test_data = test_data.astype('float32') / 255.\n",
        "\n",
        "# one-hot encode the labels - we have 10 output classes\n",
        "# so 3 -> [0 0 0 1 0 0 0 0 0 0], 5 -> [0 0 0 0 0 1 0 0 0 0] & so on\n",
        "from keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "train_labels_cat = to_categorical(train_labels,num_classes)\n",
        "test_labels_cat = to_categorical(test_labels,num_classes)\n",
        "train_labels_cat.shape, test_labels_cat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKEo1v4UqdCG"
      },
      "source": [
        "num_clients = 10\n",
        "size = train_data.shape[0] // num_clients\n",
        "from collections import defaultdict\n",
        "clients_dict = defaultdict(dict)\n",
        "client_no = 1\n",
        "for i in range(0,size*num_clients, size):\n",
        "  clients_dict[\"client_\"+str(client_no)] = {\"X\": train_data[i: i+size], \"y\": train_labels_cat[i: i+size]}\n",
        "  client_no+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbzSg0X2cSA7"
      },
      "source": [
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCXyiIOZcOKd"
      },
      "source": [
        "#create optimizer\n",
        "comms_round = 10\n",
        "lr = 0.01 \n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "optimizer = SGD(lr=lr, \n",
        "                decay=lr / comms_round, \n",
        "                momentum=0.9\n",
        "               ) \n",
        "\n",
        "#initialize global model\n",
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build(784, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNMk-A2nsI6c"
      },
      "source": [
        "def weight_scalling_factor(clients_trn_data, curr_client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = clients_trn_data[curr_client_name][\"X\"].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([clients_trn_data[client_name][\"X\"].size for client_name in client_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = clients_trn_data[curr_client_name][\"X\"].size * bs\n",
        "    return local_count/global_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyJdZ2vOsJBD"
      },
      "source": [
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "    \n",
        "        import pdb; pdb.set_trace()\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "        \n",
        "    return avg_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkg5L47msI9O"
      },
      "source": [
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot-i0b60cOVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96b988e1-2f57-408f-a517-8742f816899b"
      },
      "source": [
        "#commence global training loop\n",
        "comms_round = 10\n",
        "for comm_round in range(comms_round):\n",
        "            \n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "    \n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_dict.keys())\n",
        "    random.shuffle(client_names)\n",
        "    \n",
        "    #loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build(784, 10)\n",
        "        local_model.compile(loss=loss, \n",
        "                      optimizer=optimizer, \n",
        "                      metrics=metrics)\n",
        "        \n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "        \n",
        "        #fit local model with client's data\n",
        "        local_model.fit(clients_dict[client][\"X\"], clients_dict[client][\"y\"], epochs=15, verbose=0) \n",
        "        \n",
        "        #scale the model weights and add to list\n",
        "        \n",
        "        scaling_factor = weight_scalling_factor(clients_dict, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "        \n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "        \n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "    \n",
        "    #update global model \n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    global_acc, global_loss = test_model(test_data, test_labels_cat,  global_model, comm_round)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> <ipython-input-8-c383bd4cd182>(18)sum_scaled_weights()\n",
            "-> layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
            "(Pdb) type(grad_list_tuple)\n",
            "<class 'tuple'>\n",
            "(Pdb) type(grad_list_tuple[0])\n",
            "<class 'numpy.ndarray'>\n",
            "(Pdb) type(grad_list_tuple[1])\n",
            "<class 'numpy.ndarray'>\n",
            "(Pdb) grad_list_tuple[1].shape\n",
            "(784, 128)\n",
            "(Pdb) grad_list_tuple[0].shape\n",
            "(784, 128)\n",
            "(Pdb) n\n",
            "> <ipython-input-8-c383bd4cd182>(19)sum_scaled_weights()\n",
            "-> avg_grad.append(layer_mean)\n",
            "(Pdb) layer_mean\n",
            "<tf.Tensor: shape=(784, 128), dtype=float32, numpy=\n",
            "array([[ 0.02402751,  0.05698669, -0.02011852, ..., -0.01844793,\n",
            "        -0.03818075, -0.03779719],\n",
            "       [-0.07331424, -0.04113057,  0.0444822 , ..., -0.00572068,\n",
            "         0.03491629,  0.04130774],\n",
            "       [ 0.04353614, -0.06092962,  0.01644968, ...,  0.0261076 ,\n",
            "         0.02592262, -0.06144632],\n",
            "       ...,\n",
            "       [ 0.04288005,  0.06939675,  0.06672618, ...,  0.05533021,\n",
            "         0.04333788,  0.04640239],\n",
            "       [ 0.00245635, -0.02440473, -0.01795647, ...,  0.03528569,\n",
            "         0.04343997,  0.03759694],\n",
            "       [ 0.03249418, -0.05724914, -0.03193332, ...,  0.07364496,\n",
            "        -0.07520755,  0.00185518]], dtype=float32)>\n",
            "(Pdb) layer_mean.shape\n",
            "TensorShape([784, 128])\n",
            "(Pdb) n\n",
            "> <ipython-input-8-c383bd4cd182>(15)sum_scaled_weights()\n",
            "-> for grad_list_tuple in zip(*scaled_weight_list):\n",
            "(Pdb) avg_grad\n",
            "[<tf.Tensor: shape=(784, 128), dtype=float32, numpy=\n",
            "array([[ 0.02402751,  0.05698669, -0.02011852, ..., -0.01844793,\n",
            "        -0.03818075, -0.03779719],\n",
            "       [-0.07331424, -0.04113057,  0.0444822 , ..., -0.00572068,\n",
            "         0.03491629,  0.04130774],\n",
            "       [ 0.04353614, -0.06092962,  0.01644968, ...,  0.0261076 ,\n",
            "         0.02592262, -0.06144632],\n",
            "       ...,\n",
            "       [ 0.04288005,  0.06939675,  0.06672618, ...,  0.05533021,\n",
            "         0.04333788,  0.04640239],\n",
            "       [ 0.00245635, -0.02440473, -0.01795647, ...,  0.03528569,\n",
            "         0.04343997,  0.03759694],\n",
            "       [ 0.03249418, -0.05724914, -0.03193332, ...,  0.07364496,\n",
            "        -0.07520755,  0.00185518]], dtype=float32)>]\n",
            "(Pdb) c\n",
            "> <ipython-input-8-c383bd4cd182>(18)sum_scaled_weights()\n",
            "-> layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
            "(Pdb) avg_grad\n",
            "[<tf.Tensor: shape=(784, 128), dtype=float32, numpy=\n",
            "array([[ 0.02402751,  0.05698669, -0.02011852, ..., -0.01844793,\n",
            "        -0.03818075, -0.03779719],\n",
            "       [-0.07331424, -0.04113057,  0.0444822 , ..., -0.00572068,\n",
            "         0.03491629,  0.04130774],\n",
            "       [ 0.04353614, -0.06092962,  0.01644968, ...,  0.0261076 ,\n",
            "         0.02592262, -0.06144632],\n",
            "       ...,\n",
            "       [ 0.04288005,  0.06939675,  0.06672618, ...,  0.05533021,\n",
            "         0.04333788,  0.04640239],\n",
            "       [ 0.00245635, -0.02440473, -0.01795647, ...,  0.03528569,\n",
            "         0.04343997,  0.03759694],\n",
            "       [ 0.03249418, -0.05724914, -0.03193332, ...,  0.07364496,\n",
            "        -0.07520755,  0.00185518]], dtype=float32)>]\n",
            "(Pdb) len(avg_grad)\n",
            "1\n",
            "(Pdb) c\n",
            "> <ipython-input-8-c383bd4cd182>(17)sum_scaled_weights()\n",
            "-> import pdb; pdb.set_trace()\n",
            "(Pdb) len(avg_grad)\n",
            "2\n",
            "(Pdb) avg_grad[1]\n",
            "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([ 2.59134378e-02, -8.81082378e-05,  2.27866285e-02, -9.62291472e-03,\n",
            "        5.97830527e-02,  1.99475624e-02,  6.79496899e-02,  3.08512640e-03,\n",
            "        1.64433364e-02,  4.78113070e-02, -2.19456665e-02,  4.41752095e-03,\n",
            "        1.01700425e-02,  7.17992708e-02, -8.06648401e-04, -3.54395073e-04,\n",
            "        2.35981513e-02,  3.87516767e-02,  2.26660762e-02, -4.26548487e-03,\n",
            "       -6.51280352e-05, -1.00387856e-02, -5.52932452e-03,  1.23109086e-03,\n",
            "        2.26255804e-02, -5.26521774e-03,  3.68272630e-03,  1.38952583e-03,\n",
            "        3.38426717e-02,  4.84693721e-02,  5.21594472e-02,  1.63538160e-03,\n",
            "        5.97981513e-02,  1.14850057e-02,  3.16192359e-02,  4.63576242e-02,\n",
            "        3.86018902e-02, -1.64985633e-03, -2.67921202e-02,  2.44393945e-02,\n",
            "        6.45338297e-02, -3.43102254e-02, -2.12566480e-02, -4.74622997e-04,\n",
            "        5.70354750e-03,  1.45830279e-02, -6.04212731e-02,  2.23309509e-02,\n",
            "        2.53754407e-02,  2.16832738e-02, -1.20052900e-02,  9.95707233e-03,\n",
            "        4.05896232e-02,  7.42548564e-03,  6.66941702e-02, -1.57872029e-02,\n",
            "        5.20983636e-02,  4.39518467e-02,  2.10396294e-02,  2.81932093e-02,\n",
            "        2.33133007e-02, -1.29737947e-02,  5.37391901e-02,  1.98637731e-02,\n",
            "        1.63876154e-02,  9.10542905e-03, -8.74395017e-03, -2.48186570e-03,\n",
            "        1.64272245e-02,  3.63946743e-02, -2.55286018e-03,  3.34901474e-02,\n",
            "        4.55673561e-02,  3.46425269e-03, -1.12378718e-02,  3.60716544e-02,\n",
            "        1.79057587e-02,  1.84284989e-02,  4.46685869e-03, -7.93570001e-03,\n",
            "       -1.92394946e-02, -1.11604659e-02,  1.85797680e-02,  6.42334148e-02,\n",
            "        5.30925542e-02, -4.00182419e-03, -3.30748409e-02,  1.31769124e-02,\n",
            "        4.65952270e-02,  5.72826294e-03,  9.40122083e-02,  2.54854951e-02,\n",
            "        1.01827616e-02,  9.92012862e-03,  7.45350271e-02,  4.13205922e-02,\n",
            "        9.95098986e-03,  1.02842078e-02, -4.08942886e-02,  1.02801556e-02,\n",
            "        4.14624438e-03,  2.21003555e-02, -2.26521003e-03,  2.66906694e-02,\n",
            "        3.09382449e-03,  8.21498968e-03,  4.44972329e-02, -7.19187688e-03,\n",
            "        1.86680462e-02,  3.10166944e-02, -9.08826885e-04,  2.78652273e-02,\n",
            "        5.12946509e-02,  3.88148353e-02, -1.19057542e-03,  5.98744415e-02,\n",
            "        5.86590776e-03,  1.84474699e-02,  7.07194814e-03, -6.94801845e-03,\n",
            "        2.87562422e-03, -6.74445927e-03,  8.17139540e-03, -8.07073712e-03,\n",
            "       -8.64081923e-03,  4.93378006e-02, -7.94600882e-03,  5.52433655e-02],\n",
            "      dtype=float32)>\n",
            "(Pdb) avg_grad[0].shape\n",
            "TensorShape([784, 128])\n",
            "(Pdb) type(avg_grad[0])\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao3WDZmmcOYI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "d8137504-9515-4cf7-edca-d3f09664e51c"
      },
      "source": [
        "smlp_SGD = SimpleMLP()\n",
        "SGD_model = smlp_SGD.build(784, 10) \n",
        "\n",
        "SGD_model.compile(loss=loss, \n",
        "              optimizer=optimizer, \n",
        "              metrics=metrics)\n",
        "\n",
        "# fit the SGD training data to model\n",
        "_ = SGD_model.fit(train_data, train_labels_cat, epochs=15, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 2.0023 - accuracy: 0.3889\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 1.5154 - accuracy: 0.6931\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 1.1845 - accuracy: 0.7668\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.9735 - accuracy: 0.8023\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8385 - accuracy: 0.8205\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.7474 - accuracy: 0.8341\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6825 - accuracy: 0.8430\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 3s 1ms/step - loss: 0.6340 - accuracy: 0.8502\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5964 - accuracy: 0.8564\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5662 - accuracy: 0.8607\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5415 - accuracy: 0.8649\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5207 - accuracy: 0.8686\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5030 - accuracy: 0.8720\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4878 - accuracy: 0.8745\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4745 - accuracy: 0.8770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clx3GWX5cOc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f880da2-52ca-46e3-869c-d8c1252d4aa6"
      },
      "source": [
        "#test the SGD global model and print out metrics\n",
        "SGD_acc, SGD_loss = test_model(test_data, test_labels_cat, SGD_model, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comm_round: 1 | global_acc: 88.640% | global_loss: 1.70375394821167\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}