{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import cPickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_classifier(filename, clf):\n",
    "    with open(filename, 'w') as fid:\n",
    "        cPickle.dump(clf, fid)\n",
    "    \n",
    "def load_classifier(filename):\n",
    "    with open(filename, 'r') as fid:\n",
    "        clf = cPickle.load(fid)\n",
    "    return clf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data: \n",
      "\n",
      "Printing the features: \n",
      "           fea1       fea2   fea3   fea4\n",
      "873   -9.300961  55.648112 -13.36  48.72\n",
      "130  -91.671044 -19.204502 -13.36  48.72\n",
      "494   65.248453  86.028782 -13.36  48.72\n",
      "380  -10.515147 -99.723539 -13.36  48.72\n",
      "377 -109.470485 -86.250225 -13.36  48.72\n",
      "\n",
      "Printing the labels: \n",
      "   labels\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "\n",
      "Dimensions: \n",
      "Features\n",
      "(1000, 4)\n",
      "\n",
      "Labels\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Data import\n",
    "Data_file = 'data/data.csv'\n",
    "Label_file = 'data/labels.txt'\n",
    "\n",
    "print \"Reading the data: \"\n",
    "X = pd.read_csv(Data_file, delimiter = ';', header = None)\n",
    "X = shuffle(X)\n",
    "y = pd.read_csv(Label_file, sep = ' ', header = None)\n",
    "y.columns = ['labels']\n",
    "X.columns = ['fea1', 'fea2', 'fea3', 'fea4']\n",
    "print\n",
    "print \"Printing the features: \"\n",
    "print X.head()\n",
    "print\n",
    "print \"Printing the labels: \"\n",
    "print y.head()\n",
    "print\n",
    "print \"Dimensions: \"\n",
    "print \"Features\"\n",
    "print X.shape\n",
    "print\n",
    "print \"Labels\"\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Summary: \n",
      "              fea1         fea2          fea3          fea4\n",
      "count  1000.000000  1000.000000  1.000000e+03  1.000000e+03\n",
      "mean      1.191948     0.122606 -1.336000e+01  4.872000e+01\n",
      "std      71.163924    71.453898  1.368479e-13  6.753534e-13\n",
      "min    -119.948256  -119.744994 -1.336000e+01  4.872000e+01\n",
      "25%     -56.495719   -61.059189 -1.336000e+01  4.872000e+01\n",
      "50%       0.183716     1.094511 -1.336000e+01  4.872000e+01\n",
      "75%      63.873119    60.266393 -1.336000e+01  4.872000e+01\n",
      "max     119.997661   119.302587 -1.336000e+01  4.872000e+01\n",
      "\n",
      "Labels Summary: \n",
      "1    687\n",
      "0    313\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Data exploration\n",
    "print \"Feature Summary: \"\n",
    "print X.describe()\n",
    "print\n",
    "print \"Labels Summary: \"\n",
    "print y.ix[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing correlations: \n",
      "              fea1          fea2          fea3          fea4\n",
      "fea1  1.000000e+00 -5.979175e-02  1.199788e-17 -8.210390e-18\n",
      "fea2 -5.979175e-02  1.000000e+00  8.186645e-18 -3.284233e-17\n",
      "fea3  1.199788e-17  8.186645e-18  1.000000e+00 -1.000000e+00\n",
      "fea4 -8.210390e-18 -3.284233e-17 -1.000000e+00  1.000000e+00\n",
      "\n",
      "Printing correlations along p-values: \n",
      "(-0.059791749008238623, 0.058743584315117066)\n",
      "(-2.4774177941010445e-17, 1.0)\n",
      "(2.4774177941010445e-17, 1.0)\n",
      "(1.9102172413612732e-17, 1.0)\n",
      "(-1.9102172413612732e-17, 1.0)\n",
      "(-1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "#Plotting the data variables\n",
    "for num, i in enumerate(X.columns):\n",
    "    plt.plot(X[i])\n",
    "    plt.savefig('plot-col'+str(num)+'.png')\n",
    "    plt.close()\n",
    "\n",
    "print \"Printing correlations: \"\n",
    "print X.corr()\n",
    "print  \n",
    "print \"Printing correlations along p-values: \"\n",
    "print pearsonr(X.ix[:,0], X.ix[:,1])\n",
    "print pearsonr(X.ix[:,0], X.ix[:,2])\n",
    "print pearsonr(X.ix[:,0], X.ix[:,3])\n",
    "print pearsonr(X.ix[:,1], X.ix[:,2])\n",
    "print pearsonr(X.ix[:,1], X.ix[:,3])\n",
    "print pearsonr(X.ix[:,2], X.ix[:,3])\n",
    "#no correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing the features: \n",
      "         fea1      fea2  fea3  fea4\n",
      "873 -0.147521  0.777470   1.0  -1.0\n",
      "130 -1.305570 -0.270619   1.0  -1.0\n",
      "494  0.900576  1.202862   1.0  -1.0\n",
      "380 -0.164591 -1.398050   1.0  -1.0\n",
      "377 -1.555814 -1.209396   1.0  -1.0\n",
      "\n",
      "Changing to square: \n",
      "             fea1         fea2      fea3       fea4\n",
      "873     86.507874  3096.712421  178.4896  2373.6384\n",
      "130   8403.580373   368.812883  178.4896  2373.6384\n",
      "494   4257.360661  7400.951270  178.4896  2373.6384\n",
      "380    110.568314  9944.784162  178.4896  2373.6384\n",
      "377  11983.787025  7439.101377  178.4896  2373.6384\n"
     ]
    }
   ],
   "source": [
    "#Changing the features:\n",
    "#Normalizing the features\n",
    "\n",
    "print \"Normalizing the features: \"\n",
    "X_normalize = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "print X_normalize.head()\n",
    "print\n",
    "\n",
    "'''\n",
    "#Changing to logarithm\n",
    "print \"Changing to the log: \"\n",
    "X_log = X.apply(np.log)\n",
    "print X_log.head()\n",
    "print \"After filling NAs\"\n",
    "X_log = X_log.fillna(X_log.mean())\n",
    "print X_log.head()\n",
    "print \n",
    "'''\n",
    "#Changing to square\n",
    "print \"Changing to square: \"\n",
    "X_square = X.apply(np.square)\n",
    "#X_square = X_square.ix[:,0:2]\n",
    "print X_square.head()\n",
    "\n",
    "for num, i in enumerate(X_square.columns):\n",
    "    plt.plot(X_square[i])\n",
    "    plt.savefig('squared_plot-col'+str(num)+'.png')\n",
    "    plt.close()\n",
    "\n",
    "#X = X_square.ix[:,0:2]\n",
    "X = X_normalize.ix[:,0:2]\n",
    "    \n",
    "#Split into train and test- using the square of the features\n",
    "frame = pd.concat([y, X], axis = 1)\n",
    "train, test = train_test_split(frame, test_size=0.2)\n",
    "#X_train, y_train = train.ix[:,1:], train['labels']\n",
    "#X_test, y_test = test.ix[:,1:], test['labels']\n",
    "\n",
    "X_train, y_train = frame.ix[:,1:], frame['labels']\n",
    "X_test, y_test = frame.ix[:,1:], frame['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Implementing Logistic Regression from Scratch\n",
    "class LogisticRegressor(object):\n",
    "    def __init__(self, lbda):\n",
    "        #lambda for regularization\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def sigmoid_func(self, theta, x):\n",
    "        return float(1) / (1 + math.e**(-x.dot(theta)))\n",
    "    \n",
    "    def cost_func(self, theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        hypo_func = self.sigmoid_func(theta, x)\n",
    "        term1 = y*np.log(hypo_func)\n",
    "        term2 = (1-y) * np.log(1-hypo_func)\n",
    "        final = (-term1-term2) * float(1/m)\n",
    "        regularized_factor = float(self.lbda/(2*m)) * theta.T.dot(theta)\n",
    "        return final + regularized_factor\n",
    "    \n",
    "    def gradient(self, theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        temp = (self.sigmoid_func(theta, x) - np.squeeze(y))* float(1/m)\n",
    "        regularized_factor = float(self.lbda/m) * theta\n",
    "        return temp.T.dot(x) + regularized_factor\n",
    "    \n",
    "    def grad_desc(self, theta_values, X, y, alpha=.001, converge_change=.01):\n",
    "        #normalize\n",
    "        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "        cost_iter = []\n",
    "        #compute initial cost function\n",
    "        cost = self.cost_func(theta_values, X, y)\n",
    "        #tracking cost at each iterations\n",
    "        cost_iter.append([0, cost])\n",
    "        change_cost = 1\n",
    "        i = 1\n",
    "        try:\n",
    "            while change_cost > converge_change:\n",
    "                old_cost = cost\n",
    "                #theta value change\n",
    "                theta_values = theta_values - (alpha * self.gradient(theta_values, X, y))\n",
    "                #recompute cost function\n",
    "                cost = self.cost_func(theta_values, X, y)\n",
    "                cost_iter.append([i, cost])\n",
    "                change_cost = old_cost - cost\n",
    "                i+=1\n",
    "        except:\n",
    "            pass\n",
    "        return theta_values, []\n",
    "        #list(np.array(cost_iter))\n",
    "    \n",
    "    def pred_values(self, theta, X, hard=True):\n",
    "        #normalize\n",
    "        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "        pred_prob = self.sigmoid_func(theta, X)\n",
    "        pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "        if hard:\n",
    "            return pred_value\n",
    "        return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fea1    0.0\n",
      "fea2    0.0\n",
      "dtype: float64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       313\n",
      "          1       0.69      1.00      0.81       687\n",
      "\n",
      "avg / total       0.47      0.69      0.56      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegressor(0.1)\n",
    "shape = X_train.shape[1]\n",
    "betas = np.zeros(shape)\n",
    "theta_values, cost_iter = lreg.grad_desc(betas, X_train, y_train)\n",
    "print theta_values\n",
    "predicted_y = lreg.pred_values(theta_values, X_test)\n",
    "print classification_report(y_test, predicted_y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.86      0.54       313\n",
      "          1       0.86      0.41      0.55       687\n",
      "\n",
      "avg / total       0.72      0.55      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regressor: scikit-learn\n",
    "lreg_sci = SGDClassifier(loss='log', penalty='l2', )\n",
    "lreg_sci.fit(X_train, y_train)\n",
    "save_classifier('Classifier_logit.pkl', lreg_sci)\n",
    "#To extract the classifier uncomment the line below\n",
    "#lreg = extract_classifier('SGDClassi(fier.pkl')\n",
    "pred = lreg_sci.predict(X_test)\n",
    "print classification_report(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
