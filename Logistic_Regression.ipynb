{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import cPickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_classifier(filename, clf):\n",
    "    with open(filename, 'w') as fid:\n",
    "        cPickle.dump(clf, fid)\n",
    "    \n",
    "def load_classifier(filename):\n",
    "    with open(filename, 'r') as fid:\n",
    "        clf = cPickle.load(fid)\n",
    "    return clf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data: \n",
      "\n",
      "Printing the features: \n",
      "           fea1        fea2   fea3   fea4\n",
      "912   12.390153  101.751372 -13.36  48.72\n",
      "944  -38.159785  -12.747203 -13.36  48.72\n",
      "140 -112.334686   19.236099 -13.36  48.72\n",
      "110 -115.444450  -91.328414 -13.36  48.72\n",
      "841   85.872299  -29.827533 -13.36  48.72\n",
      "\n",
      "Printing the labels: \n",
      "   labels\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "\n",
      "Dimensions: \n",
      "Features\n",
      "(1000, 4)\n",
      "\n",
      "Labels\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "#Data import\n",
    "Data_file = 'data/data.csv'\n",
    "Label_file = 'data/labels.txt'\n",
    "\n",
    "print \"Reading the data: \"\n",
    "X = pd.read_csv(Data_file, delimiter = ';', header = None)\n",
    "X = shuffle(X)\n",
    "y = pd.read_csv(Label_file, sep = ' ', header = None)\n",
    "y.columns = ['labels']\n",
    "X.columns = ['fea1', 'fea2', 'fea3', 'fea4']\n",
    "print\n",
    "print \"Printing the features: \"\n",
    "print X.head()\n",
    "print\n",
    "print \"Printing the labels: \"\n",
    "print y.head()\n",
    "print\n",
    "print \"Dimensions: \"\n",
    "print \"Features\"\n",
    "print X.shape\n",
    "print\n",
    "print \"Labels\"\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Summary: \n",
      "              fea1         fea2          fea3          fea4\n",
      "count  1000.000000  1000.000000  1.000000e+03  1.000000e+03\n",
      "mean      1.191948     0.122606 -1.336000e+01  4.872000e+01\n",
      "std      71.163924    71.453898  1.368479e-13  6.753534e-13\n",
      "min    -119.948256  -119.744994 -1.336000e+01  4.872000e+01\n",
      "25%     -56.495719   -61.059189 -1.336000e+01  4.872000e+01\n",
      "50%       0.183716     1.094511 -1.336000e+01  4.872000e+01\n",
      "75%      63.873119    60.266393 -1.336000e+01  4.872000e+01\n",
      "max     119.997661   119.302587 -1.336000e+01  4.872000e+01\n",
      "\n",
      "Labels Summary: \n",
      "1    687\n",
      "0    313\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Data exploration\n",
    "print \"Feature Summary: \"\n",
    "print X.describe()\n",
    "print\n",
    "print \"Labels Summary: \"\n",
    "print y.ix[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing correlations: \n",
      "              fea1          fea2          fea3          fea4\n",
      "fea1  1.000000e+00 -5.979175e-02  5.313942e-18  1.453643e-17\n",
      "fea2 -5.979175e-02  1.000000e+00  2.216183e-17 -1.715844e-17\n",
      "fea3  5.313942e-18  2.216183e-17  1.000000e+00 -1.000000e+00\n",
      "fea4  1.453643e-17 -1.715844e-17 -1.000000e+00  1.000000e+00\n",
      "\n",
      "Printing correlations along p-values: \n",
      "(-0.059791749008238623, 0.058743584315117066)\n",
      "(-1.9180008728524217e-17, 1.0)\n",
      "(1.9180008728524217e-17, 1.0)\n",
      "(1.7510324712478338e-17, 1.0)\n",
      "(-1.7510324712478338e-17, 1.0)\n",
      "(-1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "#Plotting the data variables\n",
    "for num, i in enumerate(X.columns):\n",
    "    plt.plot(X[i])\n",
    "    plt.savefig('plot-col'+str(num)+'.png')\n",
    "    plt.close()\n",
    "\n",
    "print \"Printing correlations: \"\n",
    "print X.corr()\n",
    "print  \n",
    "print \"Printing correlations along p-values: \"\n",
    "print pearsonr(X.ix[:,0], X.ix[:,1])\n",
    "print pearsonr(X.ix[:,0], X.ix[:,2])\n",
    "print pearsonr(X.ix[:,0], X.ix[:,3])\n",
    "print pearsonr(X.ix[:,1], X.ix[:,2])\n",
    "print pearsonr(X.ix[:,1], X.ix[:,3])\n",
    "print pearsonr(X.ix[:,2], X.ix[:,3])\n",
    "#no correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing the features: \n",
      "         fea1      fea2  fea3  fea4\n",
      "912  0.157437  1.423010   1.0  -1.0\n",
      "944 -0.553250 -0.180204   1.0  -1.0\n",
      "140 -1.596082  0.267628   1.0  -1.0\n",
      "110 -1.639802 -1.280501   1.0  -1.0\n",
      "841  1.190529 -0.419363   1.0  -1.0\n",
      "\n",
      "Changing to square: \n",
      "             fea1          fea2      fea3       fea4\n",
      "912    153.515895  10353.341726  178.4896  2373.6384\n",
      "944   1456.169217    162.491187  178.4896  2373.6384\n",
      "140  12619.081722    370.027500  178.4896  2373.6384\n",
      "110  13327.420937   8340.879253  178.4896  2373.6384\n",
      "841   7374.051675    889.681702  178.4896  2373.6384\n"
     ]
    }
   ],
   "source": [
    "#Changing the features:\n",
    "#Normalizing the features\n",
    "\n",
    "print \"Normalizing the features: \"\n",
    "X_normalize = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "print X_normalize.head()\n",
    "print\n",
    "\n",
    "'''\n",
    "#Changing to logarithm\n",
    "print \"Changing to the log: \"\n",
    "X_log = X.apply(np.log)\n",
    "print X_log.head()\n",
    "print \"After filling NAs\"\n",
    "X_log = X_log.fillna(X_log.mean())\n",
    "print X_log.head()\n",
    "print \n",
    "'''\n",
    "#Changing to square\n",
    "print \"Changing to square: \"\n",
    "X_square = X.apply(np.square)\n",
    "#X_square = X_square.ix[:,0:2]\n",
    "print X_square.head()\n",
    "\n",
    "for num, i in enumerate(X_square.columns):\n",
    "    plt.plot(X_square[i])\n",
    "    plt.savefig('squared_plot-col'+str(num)+'.png')\n",
    "    plt.close()\n",
    "\n",
    "#X = X_square.ix[:,0:2]\n",
    "X = X_normalize.ix[:,0:2]\n",
    "    \n",
    "#Split into train and test- using the square of the features\n",
    "frame = pd.concat([y, X], axis = 1)\n",
    "train, test = train_test_split(frame, test_size=0.2)\n",
    "#X_train, y_train = train.ix[:,1:], train['labels']\n",
    "#X_test, y_test = test.ix[:,1:], test['labels']\n",
    "\n",
    "X_train, y_train = frame.ix[:,1:], frame['labels']\n",
    "X_test, y_test = frame.ix[:,1:], frame['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Implementing Logistic Regression from Scratch\n",
    "class LogisticRegressor(object):\n",
    "    def __init__(self, lbda):\n",
    "        #lambda for regularization\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def sigmoid_func(self, theta, x):\n",
    "        return float(1) / (1 + math.e**(-x.dot(theta)))\n",
    "    \n",
    "    def cost_func(self, theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        hypo_func = self.sigmoid_func(theta, x)\n",
    "        term1 = y*np.log(hypo_func)\n",
    "        term2 = (1-y) * np.log(1-hypo_func)\n",
    "        final = (-term1-term2) * float(1/m)\n",
    "        regularized_factor = float(self.lbda/(2*m)) * theta.T.dot(theta)\n",
    "        return final + regularized_factor\n",
    "    \n",
    "    def gradient(self, theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        temp = (self.sigmoid_func(theta, x) - np.squeeze(y))* float(1/m)\n",
    "        regularized_factor = float(self.lbda/m) * theta\n",
    "        return temp.T.dot(x) + regularized_factor\n",
    "    \n",
    "    def grad_desc(self, theta_values, X, y, alpha=.001, converge_change=.01):\n",
    "        #normalize\n",
    "        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "        cost_iter = []\n",
    "        #compute initial cost function\n",
    "        cost = self.cost_func(theta_values, X, y)\n",
    "        #tracking cost at each iterations\n",
    "        cost_iter.append([0, cost])\n",
    "        change_cost = 1\n",
    "        i = 1\n",
    "        try:\n",
    "            while change_cost > converge_change:\n",
    "                old_cost = cost\n",
    "                #theta value change\n",
    "                theta_values = theta_values - (alpha * self.gradient(theta_values, X, y))\n",
    "                #recompute cost function\n",
    "                cost = self.cost_func(theta_values, X, y)\n",
    "                cost_iter.append([i, cost])\n",
    "                change_cost = old_cost - cost\n",
    "                i+=1\n",
    "        except:\n",
    "            pass\n",
    "        return theta_values, []\n",
    "        #list(np.array(cost_iter))\n",
    "    \n",
    "    def pred_values(self, theta, X, hard=True):\n",
    "        #normalize\n",
    "        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "        pred_prob = self.sigmoid_func(theta, X)\n",
    "        pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "        if hard:\n",
    "            return pred_value\n",
    "        return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-f1730f8e8cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtheta_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtheta_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredicted_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-b767eb8a2737>\u001b[0m in \u001b[0;36mpred_values\u001b[0;34m(self, theta, X, hard)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mpred_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_prob\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-b767eb8a2737>\u001b[0m in \u001b[0;36msigmoid_func\u001b[0;34m(self, theta, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcost_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Shelly/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mlvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mrvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m                 raise ValueError('Dot product shape mismatch, %s vs %s' %\n\u001b[1;32m    757\u001b[0m                                  (lvals.shape, rvals.shape))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegressor(0.1)\n",
    "shape = X_train.shape[1]\n",
    "betas = np.zeros(shape)\n",
    "theta_values, cost_iter = lreg.grad_desc(betas, X_train, y_train)\n",
    "print theta_values\n",
    "predicted_y = lreg.pred_values(theta_values, X_test)\n",
    "print classification_report(y_test, predicted_y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regressor: scikit-learn\n",
    "lreg_sci = SGDClassifier(loss='log', penalty='l2', )\n",
    "lreg_sci.fit(X_train, y_train)\n",
    "save_classifier('Classifier_logit.pkl', lreg_sci)\n",
    "#To extract the classifier uncomment the line below\n",
    "#lreg = extract_classifier('SGDClassi(fier.pkl')\n",
    "pred = lreg_sci.predict(X_test)\n",
    "print classification_report(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
